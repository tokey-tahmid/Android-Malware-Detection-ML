# -*- coding: utf-8 -*-
"""

#*LOADING DATASET*
"""

import zipfile 

local_dir='/content/drive/MyDrive/Drebin Dataset/feature_vectors.zip'
zip_ref=zipfile.ZipFile(local_dir,'r')
zip_ref.extractall('/tmp/feature_vectors')
zip_ref.close

import os
PATH = '/tmp/feature_vectors/feature_vectors/'
os.listdir(PATH)

"""#*PRE-PROCESS DATA*"""

FEATURES_SET = {
    "feature": 1,
    "permission": 2,
    "api_call": 3,    
    "activity": 3,
    "service_receiver": 3,
    "service": 3,
    "intent": 4,    
    "provider": 5,
    "real_permission": 6,
    "url": 7,
    "call": 8
}


def count_feature_set(lines):
    """
    Count how many features belong to a specific set
    :param lines: features in the text file
    :return:
    """
    features_map = {x: 0 for x in range(1, 9)}
    for l in lines:
        if l != "\n":
            set = l.split("::")[0]
            features_map[FEATURES_SET[set]] += 1
    features = []
    for i in range(1, 9):
        features.append(features_map[i])
    return features

"""
Reading function for each of the classification problems
read - malware vs non malware
read_multiclass - family classification
Inside each function, customize your data path
"""
import numpy as np
import sys
from os import listdir
from os.path import isfile, join


def read(LOAD_DATA=False):
    if LOAD_DATA:
        print ("Previous data not loaded. Attempt to read data ...")
        #mypath = "data/drebin/feature_vectors"
        mypath = PATH
        onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]

        print ("Reading csv file for ground truth ...")
        ground_truth = np.loadtxt("/content/drive/MyDrive/Drebin Dataset/sha256_family.csv", delimiter=",", skiprows=1, dtype=str)
        # print ground_truth.shape
        # families = np.unique(ground_truth[:, 1])
        # print families
        # print len(families)

        print ("Reading positive and negative texts ...")
        pos = []
        neg = []
        for virus in onlyfiles:
            if virus in ground_truth[:, 0]:
                pos.append(virus)
            else:
                if len(neg) < 5560:
                    neg.append(virus)

        print ("Extracting features ...")
        x = []
        y = []
        for text_file in pos:
            sys.stdin = open("%s/%s" % (mypath, text_file))
            features = sys.stdin.readlines()
            sample = count_feature_set(features)
            x.append(sample)
            y.append(1)

        for text_file in neg:
            sys.stdin = open("%s/%s" % (mypath, text_file))
            features = sys.stdin.readlines()
            sample = count_feature_set(features)
            x.append(sample)
            y.append(0)

        print ("Data is read successfully:")
        x = np.array(x)
        y = np.array(y)
        print (x.shape, y.shape)

        print ("Saving data under data_numpy directory ...")
        with open("/tmp/x_all.npy", 'wb') as f:
          np.save(f, x)
        with open("/tmp/y_all.npy", 'wb') as f:
          np.save(f, y)

        return x, y
    else:
        print ("Loading previous data ...")
        x_ = np.load("/tmp/x_all.npy")
        y_ = np.load("/tmp/y_all.npy")
        print (x_.shape, y_.shape)
        # print x == x_, y == y_
        return x_, y_


def read_multiclass(load_data=False):
    if load_data:
        print ("Previous data not loaded. Attempt to read data ...")
        #mypath = "data/drebin/feature_vectors"
        mypath = PATH
        onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]

        print ("Reading csv file for ground truth ...")
        ground_truth = np.loadtxt("/content/drive/MyDrive/Drebin Dataset/sha256_family.csv", delimiter=",", skiprows=1, dtype=str)
        families = np.unique(ground_truth[:, 1])
        classes = map_family_to_category(families)
        # print families
        # print len(families)

        print ("Reading positive texts ...")
        pos = []
        for virus in onlyfiles:
            if virus in ground_truth[:, 0]:
                pos.append(virus)

        print ("Extracting features ...")
        x = []
        y = []
        for i in range(ground_truth.shape[0]):
            sys.stdin = open("%s/%s" % (mypath, ground_truth[i, 0]))
            features = sys.stdin.readlines()
            sample = count_feature_set(features)
            x.append(sample)
            y.append(classes[ground_truth[i, 1]])

        print ("Data is read successfully:")
        x = np.array(x)
        y = np.array(y)
        print (x.shape, y.shape)

        print ("Saving data under data_numpy directory ...")
        with open("/tmp/x_multi_all.npy", 'wb') as f:
          np.save(f, x)
        with open("/tmp/y_multi_all.npy", 'wb') as f:
          np.save(f, y)

        return x, y
    else:
        print ("Loading previous data ...")
        x_ = np.load("/tmp/x_multi_all.npy")
        y_ = np.load("/tmp/y_multi_all.npy")
        print (x_.shape, y_.shape)

        return x_, y_


def map_family_to_category(families):
    out = {}
    count = 1
    for family in families:
        out[family] = count
        count += 1
    return out


if __name__ == "__main__":
    read(LOAD_DATA=True)
    read_multiclass(load_data=True)

"""#*Feature Selection (Top 3)*"""

import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier

# Build a classification task using 3 informative features
print ("Reading data ...")
X, y = read(LOAD_DATA=False)

# Build a forest and compute the feature importances
forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                                max_depth=None, max_features='sqrt', max_leaf_nodes=None,
                                min_impurity_decrease=0.0, min_impurity_split=None,
                                min_samples_leaf=1, min_samples_split=2,
                                min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,
                                oob_score=False, random_state=None, verbose=0,
                                warm_start=False)

forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(X.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()

"""#*Random Forest (Binary Classifier)*"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

TUNING = False  # Set this to False if you don't want to tune

print ("Reading data ...")
x_all, y_all = read(LOAD_DATA=False)
x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.3, random_state=42)
print (x_train.shape, y_train.shape)
print (x_test.shape, y_test.shape)

if TUNING:

    tuned_parameters = [{'n_estimators': [10, 100, 1000],
                         'max_features': ["auto", "sqrt", "log2", None]}]

    scores = ["accuracy", "f1"]

    for score in scores:
        print("# Tuning hyper-parameters for %s" % score)
        print("")

        clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5, scoring=score, n_jobs=2)
        clf.fit(x_train, y_train)

        print("Best parameters set found on development set:")
        print("")
        print(clf.best_estimator_)
        print (clf.best_params_)
        print("")
        print("Grid scores on development set:")
        print("")
        print (clf.best_score_)
        print("")

        print("Detailed classification report:")
        print("")
        print("The model is trained on the full development set.")
        print("The scores are computed on the full evaluation set.")
        print("")
        y_true, y_pred = y_test, clf.predict(x_test)
        print(classification_report(y_true, y_pred))
        print("")

else:
    models = [RandomForestClassifier(),
              RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                                     max_depth=None, max_features='sqrt', max_leaf_nodes=None,
                                     min_impurity_decrease=0.0, min_impurity_split=None,
                                     min_samples_leaf=1, min_samples_split=2,
                                     min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,
                                     oob_score=False, random_state=None, verbose=0,
                                     warm_start=False),
              RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                                     max_depth=None, max_features='sqrt', max_leaf_nodes=None,
                                     min_impurity_decrease=0.0, min_impurity_split=None,
                                     min_samples_leaf=1, min_samples_split=2,
                                     min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,
                                     oob_score=False, random_state=None, verbose=0,
                                     warm_start=False)
              ]
    for model in models:
        print ("Fitting RF ...")
        model.fit(x_train, y_train)

        print ("Evaluating ...")
        y_pred = model.predict(x_test)

        print ("Accuracy is %f." % accuracy_score(y_test, y_pred))
        print (confusion_matrix(y_test, y_pred))
        print ("Precision score is %f." % precision_score(y_test, y_pred))
        print ("Recall score is %f." % recall_score(y_test, y_pred))
        print ("F1 score is %f." % f1_score(y_test, y_pred))
        print ("-----------------------------------")

"""#*Simple Binary Classifier*"""

from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

TUNING = False  # Set this to False if you don't want to tune

print ("Reading data ...")
x_all, y_all = read(LOAD_DATA=False)
x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.3, random_state=42)
print (x_train.shape, y_train.shape)
print (x_test.shape, y_test.shape)

if TUNING:

    tuned_parameters = [{'penalty': ['l2'], 'solver': ["lbfgs", "sag"], 'C': [0.01, 0.1, 1, 10, 100]},
                        {'penalty': ['l1'], 'solver': ["liblinear", "saga"], 'C': [0.01, 0.1, 1, 10, 100]}]

    scores = ["accuracy", "f1"]

    for score in scores:
        print("# Tuning hyper-parameters for %s" % score)
        print("")

        clf = GridSearchCV(linear_model.LogisticRegression(), tuned_parameters, cv=5, scoring=score)
        clf.fit(x_train, y_train)

        print("Best parameters set found on development set:")
        print("")
        print(clf.best_estimator_)
        print (clf.best_params_)
        print("")
        print("Grid scores on development set:")
        print("")
        print (clf.best_score_)
        # for params, mean_score, scores in clf.cv_results_:
        #     print("%0.3f (+/-%0.03f) for %r"
        #           % (mean_score, scores.std() / 2, params))
        print("")

        print("Detailed classification report:")
        print("")
        print("The model is trained on the full development set.")
        print("The scores are computed on the full evaluation set.")
        print("")
        y_true, y_pred = y_test, clf.predict(x_test)
        print(classification_report(y_true, y_pred))
        print("")

else:
    models = [
        linear_model.LogisticRegression(),
        linear_model.LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
                                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
                                        penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
                                        verbose=0, warm_start=False),
        linear_model.LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
                                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
                                        penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,
                                        verbose=0, warm_start=False)
    ]
    for model in models:
        print ("Fitting logistic regression ...")
        model.fit(x_train, y_train)

        print ("Evaluating ...")
        y_pred = model.predict(x_test)

        print ("Accuracy is %f." % accuracy_score(y_test, y_pred))
        print (confusion_matrix(y_test, y_pred))
        print ("Precision score is %f." % precision_score(y_test, y_pred))
        print ("Recall score is %f." % recall_score(y_test, y_pred))
        print ("F1 score is %f." % f1_score(y_test, y_pred))
        print ("-----------------------------------")

"""#*SVM Binary Classifier*"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

TUNING = False  # Set this to False if you don't want to tune

print ("Reading data ...")
x_all, y_all = read(LOAD_DATA=False)
x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.3, random_state=42)
print (x_train.shape, y_train.shape)
print (x_test.shape, y_test.shape)

if TUNING:

    tuned_parameters = [{'gamma': [1e-3, 1e-2, 1/8],
                         'C': [1, 10, 100, 1000]}]

    scores = ["accuracy", "f1"]

    for score in scores:
        print("# Tuning hyper-parameters for %s" % score)
        print("")

        clf = GridSearchCV(SVC(kernel="rbf"), tuned_parameters, cv=5, scoring=score, n_jobs=2)
        clf.fit(x_train, y_train)

        print("Best parameters set found on development set:")
        print("")
        print(clf.best_estimator_)
        print (clf.best_params_)
        print("")
        print("Grid scores on development set:")
        print("")
        print (clf.best_score_)
        print("")

        print("Detailed classification report:")
        print("")
        print("The model is trained on the full development set.")
        print("The scores are computed on the full evaluation set.")
        print("")
        y_true, y_pred = y_test, clf.predict(x_test)
        print(classification_report(y_true, y_pred))
        print("")

else:
    models = [SVC(),
              SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,
                  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',
                  max_iter=-1, probability=False, random_state=None, shrinking=True,
                  tol=0.001, verbose=False)
              ]
    for model in models:
        print ("Fitting SVM ...")
        model.fit(x_train, y_train)

        print ("Evaluating ...")
        y_pred = model.predict(x_test)

        print ("Accuracy is %f." % accuracy_score(y_test, y_pred))
        print (confusion_matrix(y_test, y_pred))
        print ("Precision score is %f." % precision_score(y_test, y_pred))
        print ("Recall score is %f." % recall_score(y_test, y_pred))
        print ("F1 score is %f." % f1_score(y_test, y_pred))
        print ("-----------------------------------")